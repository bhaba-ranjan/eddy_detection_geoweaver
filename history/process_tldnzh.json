[{
  "history_id" : "64we2z60643",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675864050475,
  "history_end_time" : 1675864053567,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "z4nr70",
  "indicator" : "Failed"
},{
  "history_id" : "lcOItnUCwYx0",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675863935482,
  "history_end_time" : 1675863937564,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ez6rlkv6jes",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862097740,
  "history_end_time" : 1675862183941,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "mp4eys1vp2d",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1675862002845,
  "history_end_time" : 1675862002879,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "9gjodxh74m7",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1675861966385,
  "history_end_time" : 1675861966447,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "tdmbuxqntqb",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1675861950017,
  "history_end_time" : 1675861950091,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4x5ye2t1goq",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore run_epoch\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/4x5ye2t1goq/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/4x5ye2t1goq/LossFunction_LearningRate_MetricsEvaluation.py\", line 317, in mainFunction\n    train_loss, val_loss, train_m, val_m = run_epoch(\nTypeError: run_epoch() missing 1 required positional argument: 'writer'\n",
  "history_begin_time" : 1675776999423,
  "history_end_time" : 1675777003344,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "zX2cGpn22pNw",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore run_epoch\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/zX2cGpn22pNw/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/zX2cGpn22pNw/LossFunction_LearningRate_MetricsEvaluation.py\", line 317, in mainFunction\n    train_loss, val_loss, train_m, val_m = run_epoch(\nTypeError: run_epoch() missing 1 required positional argument: 'writer'\n",
  "history_begin_time" : 1675178189080,
  "history_end_time" : 1675178192991,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hgdrC1Nc2W5q",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )'''\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/hgdrC1Nc2W5q/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/hgdrC1Nc2W5q/LossFunction_LearningRate_MetricsEvaluation.py\", line 288, in mainFunction\n    writer = SummaryWriter(log_dir=tensorboard_dir)\nNameError: name 'tensorboard_dir' is not defined\n",
  "history_begin_time" : 1675178163243,
  "history_end_time" : 1675178167194,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "EjVrWlZ5v2Xq",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore run_epoch\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/EjVrWlZ5v2Xq/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/EjVrWlZ5v2Xq/LossFunction_LearningRate_MetricsEvaluation.py\", line 327, in mainFunction\n    writer,\nNameError: name 'writer' is not defined\n",
  "history_begin_time" : 1675178138168,
  "history_end_time" : 1675178142076,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LwNVlt6pocCO",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore run_epoch\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/LwNVlt6pocCO/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/LwNVlt6pocCO/LossFunction_LearningRate_MetricsEvaluation.py\", line 311, in mainFunction\n    path=checkpoint_path,\nNameError: name 'checkpoint_path' is not defined\n",
  "history_begin_time" : 1675178092294,
  "history_end_time" : 1675178096215,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TxI1bKJZkjfs",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore run_epoch\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/TxI1bKJZkjfs/LossFunction_LearningRate_MetricsEvaluation.py\", line 378, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/TxI1bKJZkjfs/LossFunction_LearningRate_MetricsEvaluation.py\", line 308, in mainFunction\n    checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nNameError: name 'tensorboard_dir' is not defined\n",
  "history_begin_time" : 1675178057401,
  "history_end_time" : 1675178061293,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "iATRJTLBw8YH",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \n#if __name__ == \"__main__\":\nmainFunction()\n  \n",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675177523150,
  "history_end_time" : 1675177526558,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "uyE0pyi1FdoS",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \n#if __name__ == \"__main__\":\n mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/uyE0pyi1FdoS/LossFunction_LearningRate_MetricsEvaluation.py\", line 378\n    mainFunction()\n                  ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675177517916,
  "history_end_time" : 1675177517971,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "y1sOfHb9l64y",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  #loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675177312674,
  "history_end_time" : 1675177316074,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dOgUCbe4T1zB",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  '''scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')'''\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675177284753,
  "history_end_time" : 1675177288179,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2aVdaIJ59nB5",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \n'''if __name__ == \"__main__\":\n  mainFunction()'''\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\n",
  "history_begin_time" : 1675177218045,
  "history_end_time" : 1675177221853,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "B2KoLmyKH0xy",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)'''\n\n  \n  \n'''if __name__ == \"__main__\":\n  mainFunction()'''\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\n",
  "history_begin_time" : 1675177192603,
  "history_end_time" : 1675177196377,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7CkNAJFdiIay",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675177154588,
  "history_end_time" : 1675177158069,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eb13HSxvomNE",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/eb13HSxvomNE/LossFunction_LearningRate_MetricsEvaluation.py\", line 237\n    def count_eddies(arr, eddy_type=\"both\"):\n                                            ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675177135115,
  "history_end_time" : 1675177135166,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dKaosD90hQp4",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/dKaosD90hQp4/LossFunction_LearningRate_MetricsEvaluation.py\", line 218\n    a2.axis(\"off\")\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675177103768,
  "history_end_time" : 1675177103822,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XC3bNPTt0kOp",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/XC3bNPTt0kOp/LossFunction_LearningRate_MetricsEvaluation.py\", line 195\n    a1.axis(\"off\")\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675177047779,
  "history_end_time" : 1675177047841,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7rp0R6TMUlr6",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/7rp0R6TMUlr6/LossFunction_LearningRate_MetricsEvaluation.py\", line 174\n    def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n                                                               ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675176994434,
  "history_end_time" : 1675176994482,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "gqowNELQz482",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n    print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/gqowNELQz482/LossFunction_LearningRate_MetricsEvaluation.py\", line 172\n    print('after run_epoch')\n                            ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675176988333,
  "history_end_time" : 1675176988379,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AZCfdpKQtTlA",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/AZCfdpKQtTlA/LossFunction_LearningRate_MetricsEvaluation.py\", line 172\n    print('after run_epoch')\n                            ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675176967451,
  "history_end_time" : 1675176967497,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8161HWuQjQsg",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)'''\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/8161HWuQjQsg/LossFunction_LearningRate_MetricsEvaluation.py\", line 376, in <module>\n    mainFunction()\nNameError: name 'mainFunction' is not defined\n",
  "history_begin_time" : 1675176842736,
  "history_end_time" : 1675176846510,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Qo9gyU1y4MQ4",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)'''\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/Qo9gyU1y4MQ4/LossFunction_LearningRate_MetricsEvaluation.py\", line 78\n    if torch.cuda.is_available():  # move metrics to the same device as model\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675176829818,
  "history_end_time" : 1675176829871,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Aou5GWUrfsTY",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)'''\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/Aou5GWUrfsTY/LossFunction_LearningRate_MetricsEvaluation.py\", line 55\n    metrics = [\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675176747107,
  "history_end_time" : 1675176747155,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "6IdT5JZHlJWu",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)'''\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/6IdT5JZHlJWu/LossFunction_LearningRate_MetricsEvaluation.py\", line 45\n    def get_metrics(N, sync=False):\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675176698549,
  "history_end_time" : 1675176698596,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4owTZCkWxhWy",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/4owTZCkWxhWy/LossFunction_LearningRate_MetricsEvaluation.py\", line 45\n    def get_metrics(N, sync=False):\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675176672463,
  "history_end_time" : 1675176672508,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "evo0t9qC95BU",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\n'''def mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()'''",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\n",
  "history_begin_time" : 1675176521392,
  "history_end_time" : 1675176525734,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "RZmoRCaMCDkX",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175240821,
  "history_end_time" : 1675175244138,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "HmzWNXrG0nqZ",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n#mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/HmzWNXrG0nqZ/LossFunction_LearningRate_MetricsEvaluation.py\", line 375\n    \n    ^\nIndentationError: expected an indented block\n",
  "history_begin_time" : 1675175234490,
  "history_end_time" : 1675175234536,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "l2j9rlKMujCD",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n # mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/l2j9rlKMujCD/LossFunction_LearningRate_MetricsEvaluation.py\", line 375\n    \n    ^\nIndentationError: expected an indented block\n",
  "history_begin_time" : 1675175221742,
  "history_end_time" : 1675175221792,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nqg0z4Ia1HYJ",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  #torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175207242,
  "history_end_time" : 1675175210700,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "APlGPw1KdXd9",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  #model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175195967,
  "history_end_time" : 1675175199333,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1GbWwhvyWr9n",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  #writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175181392,
  "history_end_time" : 1675175184641,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "g2pl172A1hmM",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  #add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175166874,
  "history_end_time" : 1675175170165,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "v1W6nC2ODCkq",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  '''metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }'''\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175154017,
  "history_end_time" : 1675175157366,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "IVYP9nsoKaSb",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  '''hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }'''\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175137263,
  "history_end_time" : 1675175140571,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "lcNHriYU2Pnx",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      '''if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break'''\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175118738,
  "history_end_time" : 1675175122120,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AyDeRpZymqJo",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      #early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175103330,
  "history_end_time" : 1675175106608,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2sWrCpObXHRn",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      #early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175091950,
  "history_end_time" : 1675175095232,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ZkCwxOaCAoyI",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      #progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175068081,
  "history_end_time" : 1675175071433,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7UB4stMSCICn",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      #val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175052563,
  "history_end_time" : 1675175055827,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LfPBmZn0T1iu",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n     # train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175035134,
  "history_end_time" : 1675175038455,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nv5jQojYKpKH",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675175025150,
  "history_end_time" : 1675175028461,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RSvdtkU4jctg",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  '''for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )'''\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/RSvdtkU4jctg/LossFunction_LearningRate_MetricsEvaluation.py\", line 329\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675175004993,
  "history_end_time" : 1675175005044,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pvL2yKJ0iIGT",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  #progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174988046,
  "history_end_time" : 1675174991353,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bPaTYMbZqrfF",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174972382,
  "history_end_time" : 1675174975725,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jnSzVwxcEzkZ",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174955039,
  "history_end_time" : 1675174958313,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eiww35770pGH",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  '''num_epochs = 5'''\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174938116,
  "history_end_time" : 1675174941449,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hwFyyyLGbIhz",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  '''loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5'''\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174920649,
  "history_end_time" : 1675174923927,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "USh39BsVCGrm",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  '''def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)'''\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174892752,
  "history_end_time" : 1675174896093,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "cudZDGR3s8s1",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  '''def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3'''\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174869004,
  "history_end_time" : 1675174872319,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AQhKXIqwYuBT",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  '''def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')'''\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174837453,
  "history_end_time" : 1675174840786,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PUdzGUqqNMl1",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  #random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174800278,
  "history_end_time" : 1675174803602,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3R9EQM7mezu7",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  #num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174770547,
  "history_end_time" : 1675174773835,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xoJNLc55UMKE",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  #writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore tensorboard dir\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/xoJNLc55UMKE/LossFunction_LearningRate_MetricsEvaluation.py\", line 374, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/xoJNLc55UMKE/LossFunction_LearningRate_MetricsEvaluation.py\", line 126, in mainFunction\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\nNameError: name 'writer' is not defined\n",
  "history_begin_time" : 1675174750727,
  "history_end_time" : 1675174754396,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JkTgOPyqk0Pi",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  \"\"\"tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\"\"\"\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore tensorboard dir\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/JkTgOPyqk0Pi/LossFunction_LearningRate_MetricsEvaluation.py\", line 374, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/JkTgOPyqk0Pi/LossFunction_LearningRate_MetricsEvaluation.py\", line 124, in mainFunction\n    writer = SummaryWriter(log_dir=tensorboard_dir)\nNameError: name 'tensorboard_dir' is not defined\n",
  "history_begin_time" : 1675174726642,
  "history_end_time" : 1675174730351,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "i3YWfkr83GLt",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  #train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174694391,
  "history_end_time" : 1675174697734,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "S0qH2KnEkqC1",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      #train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/S0qH2KnEkqC1/LossFunction_LearningRate_MetricsEvaluation.py\", line 374, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/S0qH2KnEkqC1/LossFunction_LearningRate_MetricsEvaluation.py\", line 111, in mainFunction\n    train_metrics, val_metrics = get_metrics(num_classes)\n  File \"/Users/lakshmichetana/gw-workspace/S0qH2KnEkqC1/LossFunction_LearningRate_MetricsEvaluation.py\", line 108, in get_metrics\n    val_metrics = train_metrics.clone()\nNameError: free variable 'train_metrics' referenced before assignment in enclosing scope\n",
  "history_begin_time" : 1675174663486,
  "history_end_time" : 1675174667201,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pGl4XLf4lwIX",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n     #if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/pGl4XLf4lwIX/LossFunction_LearningRate_MetricsEvaluation.py\", line 105\n    [metric.to(\"cuda\") for metric in metrics]\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675174447014,
  "history_end_time" : 1675174447058,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jG3Xbeqg24NA",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n     # if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/jG3Xbeqg24NA/LossFunction_LearningRate_MetricsEvaluation.py\", line 105\n    [metric.to(\"cuda\") for metric in metrics]\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675174439675,
  "history_end_time" : 1675174439724,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oDiCcvin1ynm",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174424606,
  "history_end_time" : 1675174428342,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Aao3Q966Ev6g",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n     metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/Aao3Q966Ev6g/LossFunction_LearningRate_MetricsEvaluation.py\", line 81\n    metrics = [\n               ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675174417734,
  "history_end_time" : 1675174417779,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5YaqKDG573gr",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n     \"\"\" metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\"\"\"\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/5YaqKDG573gr/LossFunction_LearningRate_MetricsEvaluation.py\", line 81\n    \"\"\" metrics = [\n                   ^\nIndentationError: unindent does not match any outer indentation level\n",
  "history_begin_time" : 1675174381952,
  "history_end_time" : 1675174382004,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RECUr9UPQF8A",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  \"\"\"scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\"\"\"\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675174110716,
  "history_end_time" : 1675174114141,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XH3n7uvZFm1N",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  # -- #scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "  File \"/Users/lakshmichetana/gw-workspace/XH3n7uvZFm1N/LossFunction_LearningRate_MetricsEvaluation.py\", line 61\n    optimizer,\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1675173923727,
  "history_end_time" : 1675173923772,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "venIawPxQl0C",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  # -- #optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/venIawPxQl0C/LossFunction_LearningRate_MetricsEvaluation.py\", line 374, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/venIawPxQl0C/LossFunction_LearningRate_MetricsEvaluation.py\", line 61, in mainFunction\n    optimizer,\nNameError: name 'optimizer' is not defined\n",
  "history_begin_time" : 1675173907953,
  "history_end_time" : 1675173911518,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3cR0dHMJC132",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  # -- #loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173881998,
  "history_end_time" : 1675173885259,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bjcbGe5I8Ziu",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\n# -- #from eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173864132,
  "history_end_time" : 1675173867437,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "DwCBU0jOSahs",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\n# -- #from declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173847848,
  "history_end_time" : 1675173851168,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "GmzfveJhdyxc",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\n# -- #from eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173829980,
  "history_end_time" : 1675173833262,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "MCec4mmK3fwy",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\n# -- #from tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173810645,
  "history_end_time" : 1675173813993,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "lXLTTDIbkM91",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\n# -- #import matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173787156,
  "history_end_time" : 1675173790420,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "K5dxfsDrYZ6o",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\n# -- #import cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173763919,
  "history_end_time" : 1675173767173,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4rPQUT9jaOUz",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\n# -- #from torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nbefore tensorboard dir\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/4rPQUT9jaOUz/LossFunction_LearningRate_MetricsEvaluation.py\", line 371, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/4rPQUT9jaOUz/LossFunction_LearningRate_MetricsEvaluation.py\", line 121, in mainFunction\n    writer = SummaryWriter(log_dir=tensorboard_dir)\nNameError: name 'SummaryWriter' is not defined\n",
  "history_begin_time" : 1675173744480,
  "history_end_time" : 1675173748141,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "kNM7Dv0EFT9G",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\n# -- #import datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173724946,
  "history_end_time" : 1675173728191,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "aPBTDeQuKhe8",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\nfrom eddy_import import *\n\nprint('eddy_import')\n\nfrom pytorch_local import *\n\nprint('pytorch_local')\nfrom data_utils import *\n\nprint('get_eddy_dataloader')\n# -- #import torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/aPBTDeQuKhe8/LossFunction_LearningRate_MetricsEvaluation.py\", line 369, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/aPBTDeQuKhe8/LossFunction_LearningRate_MetricsEvaluation.py\", line 106, in mainFunction\n    train_metrics, val_metrics = get_metrics(num_classes)\n  File \"/Users/lakshmichetana/gw-workspace/aPBTDeQuKhe8/LossFunction_LearningRate_MetricsEvaluation.py\", line 77, in get_metrics\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\nNameError: name 'torchmetrics' is not defined\n",
  "history_begin_time" : 1675173639783,
  "history_end_time" : 1675173643428,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Rbc9JtIrjaSf",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\nfrom eddy_import import *\n\nprint('eddy_import')\n\nfrom pytorch_local import *\n\nprint('pytorch_local')\n# -- #from data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173623545,
  "history_end_time" : 1675173626927,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JIjMgvvdQVNv",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\nfrom eddy_import import *\n\nprint('eddy_import')\n\nfrom pytorch_local import *\n\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173605344,
  "history_end_time" : 1675173608687,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "mVv0sLXGZrJM",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\n# --\n#print('start imports')\n\n#import torch\n# -- #print('torch')\n#from eddy_import import *\n\n# -- #print('eddy_import')\n\nfrom pytorch_local import *\n\n# -- #print('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\n# -- #import torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nafter scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/mVv0sLXGZrJM/LossFunction_LearningRate_MetricsEvaluation.py\", line 369, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/mVv0sLXGZrJM/LossFunction_LearningRate_MetricsEvaluation.py\", line 106, in mainFunction\n    train_metrics, val_metrics = get_metrics(num_classes)\n  File \"/Users/lakshmichetana/gw-workspace/mVv0sLXGZrJM/LossFunction_LearningRate_MetricsEvaluation.py\", line 77, in get_metrics\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\nNameError: name 'torchmetrics' is not defined\n",
  "history_begin_time" : 1675173543812,
  "history_end_time" : 1675173547461,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "6gNyH3sO8hYk",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\n# --\n#print('start imports')\n\n#import torch\n# -- #print('torch')\n#from eddy_import import *\n\n# -- #print('eddy_import')\n\nfrom pytorch_local import *\n\n# -- #print('pytorch_local')\n# -- #from data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173522495,
  "history_end_time" : 1675173525757,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AuFslVbE1PYN",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\n# --\n#print('start imports')\n\n#import torch\n# -- #print('torch')\n#from eddy_import import *\n\n# -- #print('eddy_import')\n\n# -- #from pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "pytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/AuFslVbE1PYN/LossFunction_LearningRate_MetricsEvaluation.py\", line 368, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/AuFslVbE1PYN/LossFunction_LearningRate_MetricsEvaluation.py\", line 53, in mainFunction\n    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nNameError: name 'model' is not defined\n",
  "history_begin_time" : 1675173271618,
  "history_end_time" : 1675173274857,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "svJ4PqpXbb69",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\n# --\n#print('start imports')\n\n#import torch\n# -- #print('torch')\n#from eddy_import import *\n\n# -- #print('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173258615,
  "history_end_time" : 1675173262302,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PNRqvVadKhC9",
  "history_input" : "import os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675173012292,
  "history_end_time" : 1675173015541,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "UQbwpKXDo54a",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172999801,
  "history_end_time" : 1675173003098,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Zk2jUO3Bjyuu",
  "history_input" : "import os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172978060,
  "history_end_time" : 1675172981328,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rREzUx6dUh8v",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172964749,
  "history_end_time" : 1675172968041,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NEwvFWX0B7Th",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172950640,
  "history_end_time" : 1675172953953,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nPsPoQ3Mk2eY",
  "history_input" : "import os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172799905,
  "history_end_time" : 1675172803183,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "3qmkrWfuh3cd",
  "history_input" : "import os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  #num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/3qmkrWfuh3cd/LossFunction_LearningRate_MetricsEvaluation.py\", line 364, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/3qmkrWfuh3cd/LossFunction_LearningRate_MetricsEvaluation.py\", line 54, in mainFunction\n    epochs=num_epochs,\nUnboundLocalError: local variable 'num_epochs' referenced before assignment\n",
  "history_begin_time" : 1675172784231,
  "history_end_time" : 1675172787973,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JlDjFzWkgVew",
  "history_input" : "import os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172597530,
  "history_end_time" : 1675172600849,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Vt6ujex7q1PJ",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172375882,
  "history_end_time" : 1675172380279,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "djenbxhrvkx",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "Running",
  "history_begin_time" : 1675172364487,
  "history_end_time" : 1675172364904,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sgGbmAteCWSi",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1675172328226,
  "history_end_time" : 1675172334468,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "WUYeOVsbT8XP",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1674571079747,
  "history_end_time" : 1674571083221,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "WQET1E8qu9Ig",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1674570796537,
  "history_end_time" : 1674570800050,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4rg2FLOkdpyM",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1674570587524,
  "history_end_time" : 1674570591056,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "G4QyeteK3Qvt",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nnum_epochs = 1\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/G4QyeteK3Qvt/LossFunction_LearningRate_MetricsEvaluation.py\", line 361, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/G4QyeteK3Qvt/LossFunction_LearningRate_MetricsEvaluation.py\", line 51, in mainFunction\n    epochs=num_epochs,\nUnboundLocalError: local variable 'num_epochs' referenced before assignment\n",
  "history_begin_time" : 1674570543463,
  "history_end_time" : 1674570547886,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9twxn3td47s",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/9twxn3td47s/LossFunction_LearningRate_MetricsEvaluation.py\", line 360, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/9twxn3td47s/LossFunction_LearningRate_MetricsEvaluation.py\", line 50, in mainFunction\n    epochs=num_epochs,\nUnboundLocalError: local variable 'num_epochs' referenced before assignment\n",
  "history_begin_time" : 1674567110230,
  "history_end_time" : 1674567114299,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "sxsmajx207n",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1674566395777,
  "history_end_time" : 1674566395873,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "wPFx38MQV9fY",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "start imports\ntorch\neddy_import\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\npytorch_local\nget_eddy_dataloader\ntorchmetrics\ndatetime\nSummaryWriter\ncv2\nmatplotlib.pyplot\ntqdm.auto \nend of imports\nBefore Loss Function\nAfter loss Function\nBefore scheduler initiation\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/wPFx38MQV9fY/LossFunction_LearningRate_MetricsEvaluation.py\", line 360, in <module>\n    mainFunction()\n  File \"/Users/lakshmichetana/gw-workspace/wPFx38MQV9fY/LossFunction_LearningRate_MetricsEvaluation.py\", line 50, in mainFunction\n    epochs=num_epochs,\nUnboundLocalError: local variable 'num_epochs' referenced before assignment\n",
  "history_begin_time" : 1670337731160,
  "history_end_time" : 1670337738016,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "LJ6nkn8Qx6WL",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n  def get_metrics(N, sync=False):\n      \"\"\"Get the metrics to be used in the training loop.\n      Args:\n          N (int): The number of classes.\n          sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n      Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n      \"\"\"\n      # Define metrics and move to GPU if available\n      metrics = [\n          torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n          torchmetrics.Precision(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n          torchmetrics.Recall(\n              average=None,\n              dist_sync_on_step=sync,\n              num_classes=N,\n          ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n          torchmetrics.F1Score(\n              average=\"none\",  # return F1 for each class\n              dist_sync_on_step=sync,\n              num_classes=N,\n          )\n      ]\n      if torch.cuda.is_available():  # move metrics to the same device as model\n          [metric.to(\"cuda\") for metric in metrics]\n\n      train_metrics = torchmetrics.MetricCollection(metrics)\n      val_metrics = train_metrics.clone()\n      return train_metrics, val_metrics\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  print('before tensorboard dir')\n  tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  def run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n  print('after run_epoch')\n\n  def plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n      im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n      a1.figure.canvas.draw()\n      a1.figure.canvas.flush_events()\n      a2.figure.canvas.draw()\n      a2.figure.canvas.flush_events()\n      a3.figure.canvas.draw()\n      a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n      t1 = a1.text(\n          0.5,\n          1.05,\n          f\"ADT {date}\",\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a1.transAxes,\n      )\n      # set axis off\n      a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n      mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n      mask_cyclonic = count_eddies(mask, \"cyclonic\")\n      pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n      pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n      acc = np.sum(pred == mask) / mask.size\n      im2 = a2.imshow(pred, cmap=\"viridis\")\n      t2 = a2.text(\n          0.5,\n          1.05,\n          (\n              f\"Prediction (Acc = {acc:.3f} |\"\n              f\" Num. anticyclonic = {pred_anticyclonic} |\"\n              f\" Num. cyclonic = {pred_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a2.transAxes,\n      )\n      a2.axis(\"off\")\n      im3 = a3.imshow(mask, cmap=\"viridis\")\n      t3 = a3.text(\n          0.5,\n          1.05,\n          (\n              f\"Ground Truth\"\n              f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n              f\" Num. cyclonic: {mask_cyclonic})\"\n          ),\n          size=plt.rcParams[\"axes.titlesize\"],\n          ha=\"center\",\n          transform=a3.transAxes,\n      )\n      a3.axis(\"off\")\n\n      return im1, t1, im2, t2, im3, t3\n\n\n  def count_eddies(arr, eddy_type=\"both\"):\n      mask = np.zeros(arr.shape, dtype=np.uint8)\n      if eddy_type == \"anticyclonic\":\n          mask[arr == 1] = 1\n      elif eddy_type == \"cyclonic\":\n          mask[arr == 2] = 1\n      else:\n          mask[arr > 0] = 1\n      contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n      return len(contours)\n\n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  early_stopping = EarlyStopping(\n      patience=10,\n      path=checkpoint_path,\n      min_epochs=30,\n  )\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n          writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "We assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nFound 893 files for 2000-2018.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20000101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20000410_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20000710_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20001010_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20010110_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20010420_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20010720_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20011020_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20020120_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20020430_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20020730_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20021030_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20030130_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20030501_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20030801_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20031101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20040201_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20040510_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20040810_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20041110_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20050210_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20050520_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20050820_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20051120_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20060220_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20060530_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20060830_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20061130_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20070301_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20070601_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20070901_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20071201_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20080310_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20080610_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20080910_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20081210_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20090320_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20090620_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20090920_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20091220_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20100330_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20100630_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20100930_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20101230_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20110401_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nNo filtering above 85.000000 degrees of latitude\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20110701_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20111001_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20120101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20120410_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20120710_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20121010_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20130110_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20130420_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20130720_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20131020_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20140120_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20140430_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20140730_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20141030_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20150130_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20150501_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20150801_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20151101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20160201_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20160510_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20160810_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20161110_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20170210_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20170520_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20170820_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20171120_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20180220_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20180530_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20180830_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20181130_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nNo extrema found in contour of 6 pixels in level 0.010000\nNo extrema found in contour of 4 pixels in level 0.015000\nNo extrema found in contour of 4 pixels in level 0.020000\nNo extrema found in contour of 4 pixels in level 0.055000\nNo extrema found in contour of 4 pixels in level 0.075000\nNo extrema found in contour of 4 pixels in level 0.015000\nNo extrema found in contour of 4 pixels in level -0.010000\nNo extrema found in contour of 4 pixels in level -0.010000\nNo extrema found in contour of 4 pixels in level -0.035000\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20041120_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20001020_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20100701_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20061201_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20031110_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20070610_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20110410_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20030201_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20040210_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20030510_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_20071210_vDT2021.nc\ntorch\nRead 47 samples from /Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nget_eddy_dataloader\ntorchmetrics\nSummaryWriter\nend of imports\nBefore Loss Function\nAfter loss Function\n",
  "history_begin_time" : 1670337668001,
  "history_end_time" : 1670337675568,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hdxoislev0f",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1670332342550,
  "history_end_time" : 1670332346743,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "lrb2zs3pi49",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669995103271,
  "history_end_time" : 1669995107474,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "YsySOjw9i5HI",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669761312366,
  "history_end_time" : 1669761318796,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eujWzgCxw7iv",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669740769284,
  "history_end_time" : 1669740774185,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "51PFkisLxV4t",
  "history_input" : "#loss function\nprint('start imports')\n#import torch\nprint('torch')\n#from eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669734625638,
  "history_end_time" : 1669734631022,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4HHFh6HlzX8z",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669732208607,
  "history_end_time" : 1669732214189,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "62ax2hycnxt",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1669728796029,
  "history_end_time" : 1669728796792,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rdnyhW5zmGQl",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669129244905,
  "history_end_time" : 1669129250523,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eEGn1Eeh0ZnJ",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "start imports\ntorch\neddy_import\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/LossFunction_LearningRate_MetricsEvaluation.py\", line 7, in <module>\n    from pytorch_local import *\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/pytorch_local.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/Eddy_Dataloader.py\", line 8, in <module>\n    train_loader, _ = get_eddy_dataloader(train_file, binary=binary, batch_size=batch_size)\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/joe/gw-workspace/eEGn1Eeh0ZnJ/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1669128902133,
  "history_end_time" : 1669128908754,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "NLJ8GcXVTW1C",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669128633757,
  "history_end_time" : 1669128638346,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9JpVi0dhL5qa",
  "history_input" : "#loss function\nprint('start imports')\nimport torch\nprint('torch')\nfrom eddy_import import *\nprint('eddy_import')\nfrom pytorch_local import *\nprint('pytorch_local')\nfrom data_utils import *\nprint('get_eddy_dataloader')\nimport torchmetrics\nprint('torchmetrics')\nimport datetime\nprint('datetime')\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\nimport matplotlib.pyplot as plt\nprint()\nimport numpy as np\nprint()\nfrom tqdm.auto import tqdm\nprint()\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669128592277,
  "history_end_time" : 1669128596835,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Sz2nUXDJejWd",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n#from pytorch_local import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/LossFunction_LearningRate_MetricsEvaluation.py\", line 5, in <module>\n    from get_eddy_dataloader import *\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/get_eddy_dataloader.py\", line 23, in <module>\n    train_loader, _ = get_eddy_dataloader(train_file, binary=binary, batch_size=batch_size)\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/joe/gw-workspace/Sz2nUXDJejWd/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1669128223075,
  "history_end_time" : 1669128229406,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Wb6dWKR76dT1",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch_local import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669128161211,
  "history_end_time" : 1669128166344,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CJVD7z4zKhST",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch_local import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669128112393,
  "history_end_time" : 1669128117715,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Q4ygxxvGE0UA",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/Q4ygxxvGE0UA/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\nModuleNotFoundError: No module named 'pytorch'\n",
  "history_begin_time" : 1669128099407,
  "history_end_time" : 1669128103651,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jrJiyg1g0teA",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/Eddy_Dataloader.py\", line 8, in <module>\n    train_loader, _ = get_eddy_dataloader(train_file, binary=binary, batch_size=batch_size)\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/joe/gw-workspace/jrJiyg1g0teA/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/joe/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1669127889874,
  "history_end_time" : 1669127910579,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hcjv2ylO28w3",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/hcjv2ylO28w3/LossFunction_LearningRate_MetricsEvaluation.py\", line 3, in <module>\n    from eddy_import import *\n  File \"/Users/joe/gw-workspace/hcjv2ylO28w3/eddy_import.py\", line 8, in <module>\n    from py_eddy_tracker import data\nModuleNotFoundError: No module named 'py_eddy_tracker'\n",
  "history_begin_time" : 1669127832909,
  "history_end_time" : 1669127840217,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "09CQjuHyvX9g",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669125645262,
  "history_end_time" : 1669125651654,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CQa2hpL8NmVo",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669036545578,
  "history_end_time" : 1669036551396,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "KqskJ2Et71EC",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669036070242,
  "history_end_time" : 1669036076487,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "0k1ry2weg75",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1669035394010,
  "history_end_time" : 1669035399429,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "m0zrmb4xxe3",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668660157600,
  "history_end_time" : 1668660162813,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "YXgJlM7GFEHt",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668628976070,
  "history_end_time" : 1668628981052,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "yo2sct73bki",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668628621247,
  "history_end_time" : 1668628626297,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "YeGB5ehefTzP",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nprint('Before Loss Function')\nloss_fn = torch.nn.CrossEntropyLoss()\nprint('After loss Function')\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\nprint('Before scheduler initiation')\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\nprint('after scheduler initiation')\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\nprint('before tensorboard dir')\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\nprint('after tensorboard dir')\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\nprint('before run_epoch')\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\nprint('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\nprint('model path setting')\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint('entering save option')\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668625627663,
  "history_end_time" : 1668625633844,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "65b1emuvc9m",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668624421333,
  "history_end_time" : 1668624426380,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "rr7gsoi32hx",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1668624076151,
  "history_end_time" : 1668624081636,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "v292xuqnip5",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/Eddy_Dataloader.py\", line 9, in <module>\n    val_loader, _ = get_eddy_dataloader(\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/gw-workspace/v292xuqnip5/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1668623678421,
  "history_end_time" : 1668623683283,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ovsky6ofbuv",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1668623429704,
  "history_end_time" : 1668623430636,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cc9p55zuraz",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/Eddy_Dataloader.py\", line 9, in <module>\n    val_loader, _ = get_eddy_dataloader(\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/gw-workspace/cc9p55zuraz/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-601bf215-53f9-47ac-bb7f-690c0c65c7c3/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1668623193233,
  "history_end_time" : 1668623198932,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "7vchi8whphw",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/Eddy_Dataloader.py\", line 9, in <module>\n    val_loader, _ = get_eddy_dataloader(\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/gw-workspace/7vchi8whphw/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-993f590b-4b06-4822-9239-e83745a79dc7/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1668621935997,
  "history_end_time" : 1668621940483,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ncc3tpu71an",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "",
  "history_begin_time" : 1668619504571,
  "history_end_time" : 1668619507151,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tfx543dpjm2",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/Eddy_Dataloader.py\", line 9, in <module>\n    val_loader, _ = get_eddy_dataloader(\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/gw-workspace/tfx543dpjm2/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-993f590b-4b06-4822-9239-e83745a79dc7/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1668619428382,
  "history_end_time" : 1668619433650,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ne4qaer1pok",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/LossFunction_LearningRate_MetricsEvaluation.py\", line 4, in <module>\n    from pytorch import *\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/pytorch.py\", line 4, in <module>\n    from Eddy_Dataloader import *\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/Eddy_Dataloader.py\", line 9, in <module>\n    val_loader, _ = get_eddy_dataloader(\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/data_utils.py\", line 34, in get_eddy_dataloader\n    ds, _ = get_eddy_dataset(files, binary, transform, val_split)\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/data_utils.py\", line 47, in get_eddy_dataset\n    masks, dates, _, var_filtered, lon, lat, npz_dict = read_npz_files(files)\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/data_utils.py\", line 82, in read_npz_files\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/gw-workspace/ne4qaer1pok/data_utils.py\", line 82, in <listcomp>\n    npz_contents = [np.load(file, allow_pickle=True) for file in npz_files]\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\", line 417, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/lakshmichetana/ML_eddies/dataset-satellite-sea-level-global-993f590b-4b06-4822-9239-e83745a79dc7/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz'\n",
  "history_begin_time" : 1668614704641,
  "history_end_time" : 1668614708973,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "gxvfcuqbi62",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1668611985904,
  "history_end_time" : 1668611987887,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zyunqr9706u",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "",
  "history_begin_time" : 1667848844442,
  "history_end_time" : 1667848853379,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2pj1jy25zd6",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667834282830,
  "history_end_time" : 1667834753377,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "088pe4mltps",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-11-05_08-49\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:01<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:01<08:05, 121.37s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:00<08:05, 121.37s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:00<05:59, 119.81s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:00<05:59, 119.81s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:00<04:00, 120.30s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:03<04:00, 120.30s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:03<02:01, 121.09s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [10:04<02:01, 121.09s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [10:04<00:00, 121.13s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [10:04<00:00, 120.89s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667652580406,
  "history_end_time" : 1667653189775,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ikjt8t38ye6",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-11-04_12-11\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [01:59<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [01:59<07:57, 119.27s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [03:57<07:57, 119.27s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [03:57<05:56, 118.81s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [05:54<05:56, 118.81s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [05:54<03:55, 117.78s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [07:51<03:55, 117.78s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [07:51<01:57, 117.36s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [09:48<01:57, 117.36s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [09:48<00:00, 117.36s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [09:48<00:00, 117.68s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667578275056,
  "history_end_time" : 1667578868239,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7c5seu2piu2",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-11-04_08-55\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:33<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:33<10:12, 153.25s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [05:00<10:12, 153.25s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [05:00<07:28, 149.62s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [07:32<07:28, 149.62s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [07:32<05:01, 150.98s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [10:09<05:01, 150.98s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [10:09<02:33, 153.22s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [12:40<02:33, 153.22s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [12:40<00:00, 152.22s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [12:40<00:00, 152.00s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667566515845,
  "history_end_time" : 1667567282024,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7s8k95uopet",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-53\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:13<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:13<08:54, 133.50s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:25<08:54, 133.50s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:25<06:37, 132.47s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:37<06:37, 132.47s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:37<04:24, 132.40s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:49<04:24, 132.40s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:49<02:12, 132.10s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [11:00<02:12, 132.10s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:00<00:00, 131.77s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:00<00:00, 132.08s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667188389099,
  "history_end_time" : 1667189054355,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nusguhxgk65",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667188258105,
  "history_end_time" : 1667188259056,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "j2p8DGTszAFT",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-28\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667186902947,
  "history_end_time" : 1667186916856,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "d9XlHaUTDVn1",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-27\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667186830354,
  "history_end_time" : 1667186839133,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bb5s6qiwzw8",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-20\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667186455269,
  "history_end_time" : 1667186541822,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hewa24dftz5",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-17\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667186251586,
  "history_end_time" : 1667186314350,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gal8zro2f0v",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-17\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667186238331,
  "history_end_time" : 1667186321596,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "tiy9fuuncyb",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667185684120,
  "history_end_time" : 1667186204616,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "c6589i1azau",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_23-05\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:20<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:20<09:21, 140.35s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:45<09:21, 140.35s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:45<07:08, 142.88s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [07:08<07:08, 142.88s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [07:08<04:45, 142.95s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [09:31<04:45, 142.95s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [09:31<02:22, 142.96s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [11:53<02:22, 142.96s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:53<00:00, 142.72s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:53<00:00, 142.66s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667185523813,
  "history_end_time" : 1667186242539,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fo1l97nhlv8",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667185415369,
  "history_end_time" : 1667185419426,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "96l5sy85dww",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667185393976,
  "history_end_time" : 1667185394954,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "edR4Vx2IGzd1",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-13\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-13\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:21<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:21<09:24, 141.05s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:29<09:24, 141.05s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:29<06:41, 133.75s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:38<06:41, 133.75s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:38<04:22, 131.44s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:47<04:22, 131.44s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:47<02:10, 130.43s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [10:56<02:10, 130.43s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [10:56<00:00, 130.12s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [10:56<00:00, 131.36s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667182400268,
  "history_end_time" : 1667183062683,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WiKt8Rhgo5Dh",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667182294302,
  "history_end_time" : 1667182394822,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "BB0gVVVmhkzh",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "We assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nFound 987 files for 1998-2018.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\nTraceback (most recent call last):\n    exec(code, run_globals)\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    w.start()\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.RuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\nTraceback (most recent call last):\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.RuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    exec(code, run_globals)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\n    _check_not_importing_main()\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n    raise RuntimeError('''\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exitcode = _main(fd, parent_sentinel)\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._popen = self._Popen(self)\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n    w.start()\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    raise RuntimeError('''\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n    w.start()\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    raise RuntimeError('''\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.RuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return _run_module_code(code, init_globals, run_name,\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\nTraceback (most recent call last):\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._repopulate_pool()\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    prepare(preparation_data)\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n    prepare(preparation_data)\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    raise RuntimeError('''\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exitcode = _main(fd, parent_sentinel)\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\nTraceback (most recent call last):\n    self._popen = self._Popen(self)\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    main_content = runpy.run_path(main_path,\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return _run_module_code(code, init_globals, run_name,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    prepare(preparation_data)\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    super().__init__(process_obj)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    main_content = runpy.run_path(main_path,\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    w.start()\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _fixup_main_from_path(data['init_main_from_path'])\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    main_content = runpy.run_path(main_path,\n    raise RuntimeError('''\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _fixup_main_from_path(data['init_main_from_path'])\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    super().__init__(process_obj)\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    exitcode = _main(fd, parent_sentinel)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 125, in _main\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    prepare(preparation_data)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 236, in prepare\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    _fixup_main_from_path(data['init_main_from_path'])\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 287, in _fixup_main_from_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    main_content = runpy.run_path(main_path,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    return _run_module_code(code, init_globals, run_name,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    _run_code(code, mod_globals, init_globals,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    exec(code, run_globals)\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/generateMasks_TrainTestPlots.py\", line 3, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    from importing_multiprocessor import *\n  File \"/Users/lakshmichetana/gw-workspace/Xb0gxTF1H1DY/importing_multiprocessor.py\", line 7, in <module>\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    pool = multiprocessing.Pool(processes=2)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 119, in Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    return Pool(processes, initializer, initargs, maxtasksperchild,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 212, in __init__\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    self._repopulate_pool()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 303, in _repopulate_pool\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    return self._repopulate_pool_static(self._ctx, self.Process,\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/pool.py\", line 326, in _repopulate_pool_static\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    w.start()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/process.py\", line 121, in start\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    self._popen = self._Popen(self)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/context.py\", line 284, in _Popen\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    return Popen(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    super().__init__(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_fork.py\", line 19, in __init__\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    self._launch(process_obj)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 154, in get_preparation_data\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    _check_not_importing_main()\n  File \"/Users/lakshmichetana/opt/anaconda3/lib/python3.9/multiprocessing/spawn.py\", line 134, in _check_not_importing_main\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    raise RuntimeError('''\nRuntimeError: \n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n            if __name__ == '__main__':\n                freeze_support()\n                ...\n        The \"freeze_support()\" line can be omitted if the program\n        is not going to be frozen to produce an executable.\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-03\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-03\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:  20%|██        | 1/5 [02:16<09:07, 136.81s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:   0%|          | 0/5 [02:26<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\n\nStream closed",
  "history_begin_time" : 1667182075629,
  "history_end_time" : 1667182291034,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dUJnVHgPga9c",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-06\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_22-06\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\n\nStream closed",
  "history_begin_time" : 1667182008313,
  "history_end_time" : 1667182016909,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "uHPDwrlfk2Mr",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1667181811934,
  "history_end_time" : 1667182016904,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "ahxziufi0z3",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"LossFunction_LearningRate_MetricsEvaluation.py\", line 2, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1667180727404,
  "history_end_time" : 1667180727926,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "cl0zvxvpwbc",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"LossFunction_LearningRate_MetricsEvaluation.py\", line 2, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1667180458696,
  "history_end_time" : 1667180628917,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r1tnt9by5gc",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"LossFunction_LearningRate_MetricsEvaluation.py\", line 2, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1667180398255,
  "history_end_time" : 1667180438919,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ypf1taa7tv4",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"LossFunction_LearningRate_MetricsEvaluation.py\", line 2, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1667180316132,
  "history_end_time" : 1667180359948,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uge23o2x9n4",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"LossFunction_LearningRate_MetricsEvaluation.py\", line 2, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1667180285624,
  "history_end_time" : 1667180285995,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cM2SiuJ8SpNf",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_13-45\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_13-45\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:17<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:17<09:10, 137.73s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:33<09:10, 137.73s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:33<06:49, 136.39s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:46<06:49, 136.39s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:46<04:30, 135.03s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:58<04:30, 135.03s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:58<02:13, 133.95s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [11:12<02:13, 133.95s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:12<00:00, 133.69s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:12<00:00, 134.43s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667151927946,
  "history_end_time" : 1667152605854,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IFz1nRpn5Hyo",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nfrom eddy_import import *\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\n#from loss_function import *\n#from learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\n#from metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_13-11\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_13-11\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:17<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:17<09:08, 137.09s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:29<09:08, 137.09s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:29<06:42, 134.30s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:40<06:42, 134.30s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:40<04:26, 133.00s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:52<04:26, 133.00s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:52<02:12, 132.52s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [11:07<02:12, 132.52s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:07<00:00, 133.35s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:07<00:00, 133.50s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667149910611,
  "history_end_time" : 1667150585286,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0MZEfYXUdXlu",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nfrom eddy_import import *\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\nfrom loss_function import *\nfrom learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nimport datetime\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_12-13\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_12-13\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:11<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:11<08:47, 131.80s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [04:25<08:47, 131.80s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [04:25<06:37, 132.66s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [06:38<06:37, 132.66s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [06:38<04:25, 132.90s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [08:52<04:25, 132.90s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [08:52<02:13, 133.49s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [11:06<02:13, 133.49s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:06<00:00, 133.61s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [11:06<00:00, 133.29s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1667146431728,
  "history_end_time" : 1667147103168,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "S3fKd7meNgiO",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nfrom eddy_import import *\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\nfrom loss_function import *\nfrom learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-30_12-13\n==============================================================================\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/S3fKd7meNgiO/LossFunction_LearningRate_MetricsEvaluation.py\", line 98, in <module>\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\nAttributeError: type object 'datetime.datetime' has no attribute 'datetime'\n",
  "history_begin_time" : 1667146387753,
  "history_end_time" : 1667146394909,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SPfMW1mmn8aV",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n\nfrom pytorch import *\n\nfrom get_eddy_dataloader import *\nimport torchmetrics\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\nfrom loss_function import *\nfrom learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Running",
  "history_begin_time" : 1666969412039,
  "history_end_time" : 1667136679971,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "LWnMMTd7ezd4",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nfrom pytorch import *\nfrom get_eddy_dataloader import *\nimport torchmetrics\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\n\n#Run the training loop for prescribed num_epochs\n#from loss_function import *\n#from learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\n#from metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-28_10-57\n==============================================================================\nTraceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/LWnMMTd7ezd4/LossFunction_LearningRate_MetricsEvaluation.py\", line 94, in <module>\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\nAttributeError: type object 'datetime.datetime' has no attribute 'datetime'\n",
  "history_begin_time" : 1666969063074,
  "history_end_time" : 1666969072395,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "yjVYrCEmMBP8",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n\nfrom pytorch import *\n\nfrom get_eddy_dataloader import *\nimport torchmetrics\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\nfrom loss_function import *\nfrom learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-28_10-07\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-28_10-07\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [02:55<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [02:55<11:42, 175.74s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [05:24<11:42, 175.74s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [05:24<07:59, 159.96s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [07:50<07:59, 159.96s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [07:50<05:07, 153.58s/epoch(s), train_accuracy=0.24487045, val_accuracy=0.13963155]\nTraining:  60%|██████    | 3/5 [10:20<05:07, 153.58s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965] \nTraining:  80%|████████  | 4/5 [10:20<02:31, 151.92s/epoch(s), train_accuracy=0.24561957, val_accuracy=0.1393965]\nTraining:  80%|████████  | 4/5 [12:43<02:31, 151.92s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [12:43<00:00, 149.03s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\nTraining: 100%|██████████| 5/5 [12:43<00:00, 152.79s/epoch(s), train_accuracy=0.24686049, val_accuracy=0.13930431]\n",
  "history_begin_time" : 1666966046776,
  "history_end_time" : 1666966819080,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TJaWES73ZMkF",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n\nfrom pytorch import *\n\nfrom get_eddy_dataloader import *\nimport torchmetrics\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\nfrom eddy_import import *\n\nimport datetime\nfrom torch.utils.tensorboard import SummaryWriter\n\ntensorboard_dir = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n    \"tensorboard\",\n    # add current timestamp\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n)\nwriter = SummaryWriter(log_dir=tensorboard_dir)\nprint(\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n    f\"Writing Tensorboard logs to {writer.log_dir}\"\n    f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n)\n\n\n#Train the model: Defining training loop\n\nfrom eddy_import import *\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nimport torch\n\n#Run the training loop for prescribed num_epochs\nfrom eddy_import import *\nfrom loss_function import *\nfrom learning_rate import *\nfrom trainingModel import *\nfrom tensorboard_logger import *\nfrom metrics import *\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\nnum_epochs = 5\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-28_09-46\n==============================================================================\n==============================================================================\nWriting Tensorboard logs to /Users/lakshmichetana/tensorboard/2022-10-28_09-46\n==============================================================================\nTraining:   0%|          | 0/5 [00:00<?, ?epoch(s)/s]\nTraining:   0%|          | 0/5 [03:08<?, ?epoch(s)/s, train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [03:08<12:34, 188.70s/epoch(s), train_accuracy=0.24409133, val_accuracy=0.13963155]\nTraining:  20%|██        | 1/5 [05:36<12:34, 188.70s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\nTraining:  40%|████      | 2/5 [05:36<08:14, 164.90s/epoch(s), train_accuracy=0.24418917, val_accuracy=0.13963155]\n\nStream closed",
  "history_begin_time" : 1666964807492,
  "history_end_time" : 1666965212687,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qk8N5WDp3tdO",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n\nfrom pytorch import *\n\nfrom get_eddy_dataloader import *\nimport torchmetrics\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)",
  "history_output" : "We assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nWe assume pixel position of grid is centered for /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190101_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\n",
  "history_begin_time" : 1666964491525,
  "history_end_time" : 1666964597381,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LdSFKeK8Q1pv",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\n\nfrom pytorch import *\n\nfrom get_eddy_dataloader import *\nimport torchmetrics\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n",
  "history_begin_time" : 1666964429745,
  "history_end_time" : 1666964440392,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PKQqgITqb30o",
  "history_input" : "#loss function\nimport torch\nfrom eddy_import import *\nloss_fn = torch.nn.CrossEntropyLoss()\n# TODO (homework): Try \n# loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n# learning rate for use in OneCycle scheduler\nfrom eddy_import import *\nfrom pytorch import *\nimport torch\n\ninitial_lr = 1e-6\nmax_lr = 5e-4\n\noptimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=max_lr,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs,\n    div_factor=max_lr / initial_lr,\n    pct_start=0.3,\n)\n\n#Defining and using the get_metrics function\nfrom eddy_import import *\nfrom get_eddy_dataloader import *\n\nimport torchmetrics\ndef get_metrics(N, sync=False):\n    \"\"\"Get the metrics to be used in the training loop.\n    Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n        train_metrics (MetricCollection): The metrics to be used in the training loop.\n        val_metrics (MetricCollection): The metrics to be used in validation.\n    \"\"\"\n    # Define metrics and move to GPU if available\n    metrics = [\n        torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n        torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#         torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#             average=\"micro\",\n#             dist_sync_on_step=sync,\n#             num_classes=N,\n#         ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n    ]\n    if torch.cuda.is_available():  # move metrics to the same device as model\n        [metric.to(\"cuda\") for metric in metrics]\n\n    train_metrics = torchmetrics.MetricCollection(metrics)\n    val_metrics = train_metrics.clone()\n    return train_metrics, val_metrics\n\ntrain_metrics, val_metrics = get_metrics(num_classes)",
  "history_output" : "Read 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 987 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 47 samples from /Users/lakshmichetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n",
  "history_begin_time" : 1666964244137,
  "history_end_time" : 1666964254365,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "obi7grf5q0p",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667091236276,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "usp2hjdmv84",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667091392432,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6tgyig2ncvr",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667184272263,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zt1rgc1a11v",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667184298775,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9vhu2z0wme9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667184373466,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7yiuvchnhv5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667187366247,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s08k1jmqrcb",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667562871648,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ndx7m7nmzm9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667566401766,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dqnnzb5je18",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667567846538,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "py2qpfc4p81",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667578114347,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7mzah6nw3a5",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667656268322,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ufo89qrzs36",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667834755748,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hadt6l1256d",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1667848802796,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "692kusgd2oy",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668606681626,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "98umqw60qjj",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668613059661,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "10uhvqnmq74",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668617179399,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gyqxvwn7cvi",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668619496030,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kky7gh8u7d2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668620775943,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "301moqgkt6h",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668621624259,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "26sj0ejq5e3",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668622988827,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "clq549zt9bk",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668623057323,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3bqfcpua0ag",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668659549572,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2pdj12i4v6a",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1668659939673,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r5hg0haqrqy",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1669125632387,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wnbt2n12krc",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1669729254168,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "192gc1k4g9p",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1675862231661,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "95j65wlep94",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1675862406145,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},]
