[{
  "history_id" : "juizjgainzd",
  "history_input" : "#Importing required libraries, inserting the system paths, fixing manual seeds for reproducibility\nfrom eddy_import import *\nimport os\nimport sys\nsys.path.insert(0, os.path.dirname(os.getcwd()))\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"   # useful on multi-GPU systems with multiple users\n\n# Fix manual seeds for reproducibility\nimport torch\nseed = 42\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862087067,
  "history_end_time" : 1675862183927,
  "history_notes" : null,
  "history_process" : "slycsi",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "k68huv10zg0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060727,
  "history_end_time" : 1675862183931,
  "history_notes" : null,
  "history_process" : "3hm7db",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "ps1z9qk3x4o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060729,
  "history_end_time" : 1675862183932,
  "history_notes" : null,
  "history_process" : "98bbcl",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "8vs92i4caaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060731,
  "history_end_time" : 1675862183932,
  "history_notes" : null,
  "history_process" : "ljp3lh",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "46bm2e0cv8t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060732,
  "history_end_time" : 1675862183932,
  "history_notes" : null,
  "history_process" : "w484ne",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "bgbq3xxfwc1",
  "history_input" : "#Evaluate model on training and validation sets\n#from eddy_import import *\n\n#from pytorch_local import *\n#from trainingModel import *\n#from tensorboard_logger import *\nfrom IPython.display import display, HTML\nimport torch\nfrom matplotlib.animation import ArtistAnimation\n\ndef mainFunction():\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.eval()\n  with torch.no_grad():\n      fig, ax = plt.subplots(1, 3, figsize=(25, 10))\n      artists = []\n      # loop through all SSH maps and eddy masks in 2019\n      # and run the model to generate predicted eddy masks\n      for n, (ssh_vars, seg_masks, date_indices) in enumerate(val_loader):\n          ssh_vars = ssh_vars.to(device)\n          seg_masks = seg_masks.to(device)\n          # Run the model to generate predictions\n          preds = model(ssh_vars)\n\n          # For each pixel, EddyNet outputs predictions in probabilities, \n          # so choose the channels (0, 1, or 2) with the highest prob. \n          preds = preds.argmax(dim=1)\n        \n          # Loop through all SSH maps, eddy masks, and predicted masks\n          # in this minibatch and generate a video\n          preds = preds.cpu().numpy()\n          seg_masks = seg_masks.cpu().numpy()\n          ssh_vars = ssh_vars.cpu().numpy()\n          date_indices = date_indices.cpu().numpy()\n          for i in range(len(ssh_vars)):\n              date, img, mask, pred = date_indices[i], ssh_vars[i], seg_masks[i], preds[i]\n              img1, title1, img2, title2, img3, title3 = plot_eddies_on_axes(\n                date, img, mask, pred, ax[0], ax[1], ax[2]\n              )\n              artists.append([img1, title1, img2, title2, img3, title3])\n              fig.canvas.draw()\n              fig.canvas.flush_events()\n      animation = ArtistAnimation(fig, artists, interval=200, blit=True)\n      plt.close()\n    \n  animation.save(os.path.join(tensorboard_dir, \"val_predictions.gif\"), writer=\"pillow\")\n  HTML(animation.to_jshtml())\n\n  plt.savefig(f'{figOutputFolder}/Animations.png', bbox_inches =\"tight\")\n\nif __name__ == \"__main__\":\n  mainFunction()",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862102940,
  "history_end_time" : 1675862183933,
  "history_notes" : null,
  "history_process" : "ohe0x9",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "kx0n2c6doih",
  "history_input" : "#Demonstrate the beginnings of how one can use classical computer vision techniques to recover eddy contours from the predicted segnmentation masks.\n\n#from eddy_import import *\nfrom animation import *\n\ndef mainFunction():\n  print(\"starting to import\")\n  print('importing done')\n  p = preds[0].astype(np.uint8)\n\n  print(f\"Number of anticyclonic eddies: {count_eddies(p, eddy_type='anticyclonic')}\")\n  print(f\"Number of cyclonic eddies: {count_eddies(p, eddy_type='cyclonic')}\")\n  print(f\"Number of both eddies: {count_eddies(p, eddy_type='both')}\")\n\n  # draw contours on the image\n  thr = cv2.threshold(p, 0, 1, cv2.THRESH_BINARY)[1].astype(np.uint8)\n  contours, hierarchy = cv2.findContours(thr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n  img = np.zeros(p.shape, np.uint8)\n  cv2.drawContours(img, contours, -1, (255, 255, 255), 1)\n  plt.imshow(img, cmap=\"gray\")\n  plt.axis(\"off\")\n\n  # get average contour area\n  area = 0\n  for cnt in contours:\n      area += cv2.contourArea(cnt)\n  area /= len(contours)\n  print(f\"Average contour area: {area:.2f} sq. pixels\")\n      \n  plt.savefig(f'{figOutputFolder}/EddyContours.png', bbox_inches =\"tight\")\n  \nif __name__ == \"__main__\":\n  mainFunction()\n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862109868,
  "history_end_time" : 1675862183934,
  "history_notes" : null,
  "history_process" : "kaedp2",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "lvxkohor6r6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060744,
  "history_end_time" : 1675862183934,
  "history_notes" : null,
  "history_process" : "nzmtjk",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "t6i3z4avuma",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060745,
  "history_end_time" : 1675862183934,
  "history_notes" : null,
  "history_process" : "6gs3ym",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "e6qkznqwnk2",
  "history_input" : "# Defining the start_axes, update_axes, plot_variabe  and setting the paths for eddy workflow\nfrom eddy_import import *\n\ndef start_axes(title):\n    fig = plt.figure(figsize=(13, 5))\n    ax = fig.add_axes([0.03, 0.03, 0.90, 0.94])\n    ax.set_aspect(\"equal\")\n    ax.set_title(title, weight=\"bold\")\n    return ax\n\n\ndef update_axes(ax, mappable=None):\n    ax.grid()\n    if mappable:\n        plt.colorbar(mappable, cax=ax.figure.add_axes([0.94, 0.05, 0.01, 0.9]))\n\n\ndef plot_variable(grid_object, var_name, ax_title, **kwargs):\n    ax = start_axes(ax_title)\n    m = grid_object.display(ax, var_name, **kwargs)\n    update_axes(ax, m)\n    ax.set_xlim(grid_object.x_c.min(), grid_object.x_c.max())\n    ax.set_ylim(grid_object.y_c.min(), grid_object.y_c.max())\n    return ax, m\n\ndata_root = os.path.join(os.path.expanduser(\"~\"), \"ML_eddies\")\ntrain_folder = os.path.join(data_root, \"cds_ssh_1998-2018_10day_interval\")\ntest_folder = os.path.join(data_root, \"cds_ssh_2019_10day_interval\")\n\nexample_file = os.path.join(test_folder, \"dt_global_twosat_phy_l4_20190101_vDT2021.nc\")\ndate = datetime(2019, 1, 1)\ng = RegularGridDataset(example_file, \"longitude\", \"latitude\")\n\nfigOutputFolder = '/Users/lakshmichetana/ML_Eddies_New_Data_Output/'\n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862060940,
  "history_end_time" : 1675862183935,
  "history_notes" : null,
  "history_process" : "23nut7",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "an904qcd1v6",
  "history_input" : "# setting the vmin and vmax using the eddy 'plot_variable' method\n#from eddy_paths import *\nfrom eddy_paths import figOutputFolder, plot_variable, g\nfrom copy import deepcopy\nfrom matplotlib import pyplot as plt\n\n#updated the vmin and vmax to -1 and 1\nax, m = plot_variable(\n    g,\n    \"adt\",\n    f\"ADT (m) before high-pass filter\",\n    vmin=-1,\n    vmax=1,\n)\nplt.savefig(f'{figOutputFolder}/ADT(m)_before_high-pass_filter.png', bbox_inches =\"tight\")\n#updated wavelength covered kilometers to 500 from 700\nwavelength_km = 500\n\ng_filtered = deepcopy(g)\n\ng_filtered.bessel_high_filter(\"adt\", wavelength_km)\nax, m = plot_variable(\n    g_filtered,\n    \"adt\",\n    f\"ADT (m) filtered (Final: {wavelength_km} km)\",\n    vmin=-1,\n    vmax=1,\n)\n\nplt.savefig(f'{figOutputFolder}/ADT(m)-filtered.png', bbox_inches =\"tight\")\n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862066149,
  "history_end_time" : 1675862183936,
  "history_notes" : null,
  "history_process" : "zr8vzj",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "u9hyidm7nmu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060753,
  "history_end_time" : 1675862183936,
  "history_notes" : null,
  "history_process" : "4bd5xp",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "k2rvyuj1ady",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060754,
  "history_end_time" : 1675862183936,
  "history_notes" : null,
  "history_process" : "l9f2t3",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "imk8tvu7jzi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060754,
  "history_end_time" : 1675862183937,
  "history_notes" : null,
  "history_process" : "4o6voy",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "ldzgjwt164s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060755,
  "history_end_time" : 1675862183938,
  "history_notes" : null,
  "history_process" : "j4jm66",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "zenfs38ez00",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060756,
  "history_end_time" : 1675862183938,
  "history_notes" : null,
  "history_process" : "39ur7y",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "x5b4739t0fk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060757,
  "history_end_time" : 1675862183938,
  "history_notes" : null,
  "history_process" : "uolls4",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "2e4ooqywq67",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060757,
  "history_end_time" : 1675862183938,
  "history_notes" : null,
  "history_process" : "oc42ub",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "lvjr6vl6g6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060758,
  "history_end_time" : 1675862183938,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "i5a32v6c4oy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1675862060759,
  "history_end_time" : 1675862183939,
  "history_notes" : null,
  "history_process" : "bomi2j",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "unmabebizcn",
  "history_input" : "#getting the test dates and files of training sets from 1998 - 2018 and from training set 2019 and also setting the logging level as ERROR\nfrom eddy_import import *\nfrom importing_multiprocessor import *\nfrom eddy_paths import *\nfrom eddy_plots import *\nimport logging\nfrom subset_arrays import *\n#from Generate_Masks import *\n# northern pacific (32x32 degree -> 128x128 pixels)\n\ndef funcGenerateMasks():\n  logging.getLogger(\"pet\").setLevel(logging.ERROR)\n\n    # enter the AVISO filename pattern\n    # year, month, and day in file_pattern will be filled in get_dates_and_files:\n  file_pattern = \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n  # training set: 1998 - 2018\n  train_dates, train_files = get_dates_and_files(\n      range(2000, 2019), range(1, 13), [1, 10, 20, 30], train_folder, file_pattern\n  )\n  train_adt, train_adt_filtered, train_masks = generate_masks_in_parallel(\n      train_files, train_dates\n  )\n\n\n# test set: 2019\n  test_dates, test_files = get_dates_and_files(\n      [2019], range(1, 13), [1, 10, 20, 30], test_folder, file_pattern\n  )\n  test_adt, test_adt_filtered, test_masks = generate_masks_in_parallel(\n      test_files, test_dates\n  )\n\n\n  lon_range = (-166, -134)\n  lat_range = (14, 46)\n\n  train_subset = subset_arrays(\n      train_masks,\n      train_adt,\n      train_adt_filtered,\n      train_dates,\n    lon_range,\n    lat_range,\n    plot=False,\n    resolution_deg=0.25,\n    save_folder=train_folder,\n  )\n\n  test_subset = subset_arrays(\n    test_masks,\n    test_adt,\n    test_adt_filtered,\n    test_dates,\n    lon_range,\n    lat_range,\n    plot=True,\n    resolution_deg=0.25,\n    save_folder=test_folder,\n  )\n\n  plt.savefig(f'{figOutputFolder}/Train_Test_Subset_Img.png', bbox_inches =\"tight\")\n\nif __name__ == \"__main__\":\n  funcGenerateMasks()\n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862081378,
  "history_end_time" : 1675862183940,
  "history_notes" : null,
  "history_process" : "uji5d1",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "tk9qmjjqnyu",
  "history_input" : "from file_paths import *\nfrom declaring_epochs_size import *\nfrom data_utils import get_eddy_dataloader\nfrom eddy_import import *\nimport numpy as np\nimport torch\nfrom get_eddy_dataloader import *\nfrom eddynet import EddyNet\nfrom eddy_paths import figOutputFolder\n\n# set binary = false if we want to distinguish between cyclonic and anticyclonic\nbinary = False\nnum_classes = 2 if binary else 3\ntrain_loader, _ = get_eddy_dataloader(train_file, binary=binary, batch_size=batch_size)\nval_loader, _ = get_eddy_dataloader(\n    val_file, binary=binary, batch_size=batch_size, shuffle=False\n)\n\n#Looking at the distribution of class frequencies to identify class imbalances\ntrain_masks = train_loader.dataset.masks.copy()\nclass_frequency = np.bincount(train_masks.flatten())\ntotal_pixels = sum(class_frequency)\nprint(\n    f\"Total number of pixels in training set: {total_pixels/1e6:.2f} megapixels\"\n    f\" across {len(train_masks)} SSH maps\\n\"\n    f\"Number of pixels that are not eddies: {class_frequency[0]/1e6:.2f} megapixels \"\n    f\"({class_frequency[0]/total_pixels * 100:.2f}%)\\n\"\n    f\"Number of pixels that are anticyclonic eddies: {class_frequency[1]/1e6:.2f} megapixels \"\n    f\"({class_frequency[1]/total_pixels * 100:.2f}%)\\n\"\n    f\"Number of pixels that are cyclonic eddies: {class_frequency[2]/1e6:.2f} megapixels \"\n    f\"({class_frequency[2]/total_pixels * 100:.2f}%)\\n\"\n)\n\n#Using plot_sample to visualize the dataset we just loaded.\ntrain_loader.dataset.plot_sample(N=3)\nplt.savefig(f\"{figOutputFolder}/datasetPlots\",bbox=\"tight\")\n\n#Segmentation Model:\nnum_classes = 2 if binary else 3\nmodel_name = \"eddynet\"  # we'll log this in Tensorboard\nmodel = EddyNet(num_classes, num_filters=16, kernel_size=3)\nif torch.cuda.is_available(): \n    model.to(device=\"cuda\")",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\nsh: _Pytorch.py: command not found\n",
  "history_begin_time" : 1675862092125,
  "history_end_time" : 1675862183940,
  "history_notes" : null,
  "history_process" : "qsxf3a",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "ez6rlkv6jes",
  "history_input" : "#import os\n#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n\n#loss function\nprint('start imports')\n\nimport torch\nprint('torch')\n\nfrom eddy_import import *\nprint('eddy_import')\n\nfrom pytorch_local import *\nprint('pytorch_local')\n\nfrom data_utils import *\nprint('get_eddy_dataloader')\n\nimport torchmetrics\nprint('torchmetrics')\n\nimport datetime\nprint('datetime')\n\nfrom torch.utils.tensorboard import SummaryWriter\nprint('SummaryWriter')\n\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nprint('cv2')\n\nimport matplotlib.pyplot as plt\nprint('matplotlib.pyplot')\n#import numpy as np\n\nfrom tqdm.auto import tqdm\nprint('tqdm.auto ')\n\nfrom eddy_train_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nprint('end of imports')\n\n#Run the training loop for prescribed num_epochs\nfrom declaring_epochs_size import *\nfrom eddy_train_utils import add_hparams\n\ndef get_metrics(N, sync=False):\n  \"\"\"Get the metrics to be used in the training loop.\n      Args:\n        N (int): The number of classes.\n        sync (bool): Whether to use wait for metrics to sync across devices before computing value.\n    Returns:\n          train_metrics (MetricCollection): The metrics to be used in the training loop.\n          val_metrics (MetricCollection): The metrics to be used in validation.\n  \"\"\"\n      # Define metrics and move to GPU if available\n  metrics = [\n    torchmetrics.Accuracy(dist_sync_on_step=sync, num_classes=N),\n    torchmetrics.Precision(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n        torchmetrics.Recall(\n            average=None,\n            dist_sync_on_step=sync,\n            num_classes=N,\n        ),\n#           torchmetrics.F1Score(  # TODO: Homework: verify in tensorboard that this is equivalent to accuracy\n#               average=\"micro\",\n#               dist_sync_on_step=sync,\n#               num_classes=N,\n#           ),\n        torchmetrics.F1Score(\n            average=\"none\",  # return F1 for each class\n            dist_sync_on_step=sync,\n            num_classes=N,\n        )\n  ]\n  if torch.cuda.is_available():  # move metrics to the same device as model\n      [metric.to(\"cuda\") for metric in metrics]\n\n  train_metrics = torchmetrics.MetricCollection(metrics)\n  val_metrics = train_metrics.clone()\n  return train_metrics, val_metrics\n\ndef run_epoch(\n      epoch,\n      model,\n      loss_fn,\n      optimizer,\n      scheduler,\n      train_loader,\n      val_loader,\n      train_metrics,\n      val_metrics,\n      writer,\n  ):\n      leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n      model.train()\n      # training set\n      for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n          train_loss = run_batch(\n              model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n          )\n          iter_num = epoch * len(train_loader) + batch_num\n          writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n      # validation set\n      images, preds, labels, dates = [], [], [], []\n      model.eval()\n      with torch.no_grad():\n          val_loss = num_examples = 0\n          for gvs, masks, date_indices in val_loader:\n              # continue\n              loss_, pred_batch = run_batch(\n                  model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n              )\n              val_loss += loss_\n              num_examples += np.prod(gvs.shape)\n              # keep track of images, preds, labels for plotting\n              images.append(gvs)\n              preds.append(pred_batch)\n              labels.append(masks)\n              dates.append(date_indices)\n\n      # calculate average validation loss across all samples\n      # num_examples should be equal to sum of all pixels\n      val_loss = val_loss / num_examples\n\n      # plot validation images and log to tensorboard\n      ## move images, preds, labels, dates to cpu\n      images = torch.cat(images).cpu().numpy()\n      labels = torch.cat(labels).cpu().numpy()\n      preds = torch.cat(preds).cpu().numpy()\n      dates = torch.cat(dates).cpu().numpy()\n      ## convert indices to actual dates\n      dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n      # take random images from validation set\n      if epoch == 0:\n          indices_ = np.random.choice(\n              len(images), num_plots_in_tensorboard, replace=False\n          )\n          for i, idx in enumerate(indices_):\n              random_plot_indices[i] = idx\n      fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n      for n, i in enumerate(random_plot_indices):\n          date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n          artists = plot_eddies_on_axes(\n              date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n          )\n      plt.tight_layout()\n      writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n      # Update tensorboard\n      train_m = write_metrics_to_tensorboard(\n          num_classes, train_metrics, writer, epoch, \"train\"\n      )\n      val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n      writer.add_scalar(\"train/loss\", train_loss, epoch)\n      writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n      # reset metrics after each epoch\n      train_metrics.reset()\n      val_metrics.reset()\n\n      train_m = filter_scalar_metrics(train_m)\n      val_m = filter_scalar_metrics(val_m)\n\n      return train_loss, val_loss, train_m, val_m\n      print('after run_epoch')\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n      # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n      # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n      # set axis off\n    a1.axis(\"off\")\n\n      # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n      # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)\n\n\ndef mainFunction():\n  print('Before Loss Function')\n  loss_fn = torch.nn.CrossEntropyLoss()\n  print('After loss Function')\n  # TODO (homework): Try \n  # loss_fn =    torch.nn.CrossEntropyLoss(weight=torch.Tensor(total_pixels/class_frequency))\n\n  # learning rate for use in OneCycle scheduler\n  initial_lr = 1e-6\n  max_lr = 5e-4\n  num_epochs = 1\n\n  print('Before scheduler initiation')\n  optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\n  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n      optimizer,\n      max_lr=max_lr,\n      steps_per_epoch=len(train_loader),\n      epochs=num_epochs,\n      div_factor=max_lr / initial_lr,\n      pct_start=0.3,\n  )\n  print('after scheduler initiation')\n\n  #Defining and using the get_metrics function\n\n  train_metrics, val_metrics = get_metrics(num_classes)\n\n\n#Tensor Logger\n#We use the tensor logger to log our loss and metrics throughout the training process.\n  import datetime\n  #print('before tensorboard dir')\n  '''tensorboard_dir = os.path.join(\n      os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd()))),\n      \"tensorboard\",\n      # add current timestamp\n      f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\n  )\n  writer = SummaryWriter(log_dir=tensorboard_dir)\n  print(\n      f\"{''.join(['=']*(28 + len(writer.log_dir)))}\\n\"\n      f\"Writing Tensorboard logs to {writer.log_dir}\"\n      f\"\\n{''.join(['=']*(28 + len(writer.log_dir)))}\"\n  )\n  print('after tensorboard dir')'''\n\n  #Train the model: Defining training loop\n\n  num_plots_in_tensorboard = 5\n  # will populate this later with random numbers:\n  random_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n  print('before run_epoch')\n  \n# create some aliases\n  loss, opt, sched = loss_fn, optimizer, scheduler\n  num_epochs = 5\n\n  #checkpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\n  '''early_stopping = EarlyStopping(\n      patience=10,\n     # path=checkpoint_path,\n      min_epochs=30,\n  )'''\n\n  progress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\n  for N in progress_bar:\n      train_loss, val_loss, train_m, val_m = run_epoch(\n          N,\n          model,\n          loss,\n          opt,\n          sched,\n          train_loader,\n          val_loader,\n          train_metrics,\n          val_metrics,\n         # writer,\n      )\n\n      # update progress bar\n      train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n      val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n      progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n      # early stopping when validation loss stops improving\n      early_stopping.path = checkpoint_path.format(epoch=N)\n      early_stopping(val_loss, model)\n      if early_stopping.early_stop:\n          print(\n              f\"Early stopping at epoch {N}\"\n              f\" with validation loss {val_loss:.3f}\"\n              f\" and training loss {train_loss:.3f}\"\n          )\n          break\n\n      # TODO (homework): save checkpoint every 10 epochs\n\n  # add hyperparameters and corresponding results to tensorboard HParams table\n  hparam_dict = {\n      \"backbone\": model_name,\n      \"num_epochs\": num_epochs,\n      \"batch_size\": batch_size,\n      \"num_classes\": num_classes,\n      \"binary_mask\": binary,\n      \"optimizer\": optimizer.__class__.__name__,\n      \"max_lr\": max_lr,\n      \"loss_function\": loss_fn.__class__.__name__,\n  }\n  metrics_dict = {\n      \"train/end_epoch\": N,\n      \"train/loss\": train_loss,\n      \"train/Accuracy\": train_m[\"Accuracy\"],\n      \"val/loss\": val_loss,\n      \"val/Accuracy\": val_m[\"Accuracy\"],\n  }\n  add_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\n  writer.close()\n\n  print('model path setting')\n  # save model to tensorboard folder\n  model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\n  print('entering save option')\n  torch.save(model.state_dict(), model_path)\n\n  \n  \nif __name__ == \"__main__\":\n  mainFunction()\n  \n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862097740,
  "history_end_time" : 1675862183941,
  "history_notes" : null,
  "history_process" : "tldnzh",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "4wt0jvbzf88",
  "history_input" : "# setting the vmin and vmax using the eddy 'plot_variable' method\nfrom eddy_paths import *\nfrom copy import deepcopy\nfrom matplotlib import pyplot as plt\n\n#updated the vmin and vmax to -1 and 1\nax, m = plot_variable(\n    g,\n    \"adt\",\n    f\"ADT (m) before high-pass filter\",\n    vmin=-5,\n    vmax=5,\n)\nplt.savefig(f'{figOutputFolder}/ADT(m)_before_high-pass_filter_with_updatedVminVmax&Wavelength_KM.png', bbox_inches =\"tight\")\n#updated wavelength covered kilometers to 100 from 700\nwavelength_km = 100\n\ng_filtered = deepcopy(g)\n\ng_filtered.bessel_high_filter(\"adt\", wavelength_km)\nax, m = plot_variable(\n    g_filtered,\n    \"adt\",\n    f\"ADT (m) filtered (Final: {wavelength_km} km)\",\n    vmin=-5,\n    vmax=5,\n)\n\nplt.savefig(f'{figOutputFolder}/ADT(m)-filtered_with_updatedVminVmax&Wavelength_KM.png', bbox_inches =\"tight\")\n",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862070950,
  "history_end_time" : 1675862183945,
  "history_notes" : null,
  "history_process" : "k3gm1y",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "pndy3ouiiml",
  "history_input" : "#code for plotting segmentation masks, antcyclonic display, cyclonic display and updating the axis\nfrom eddy_plots import *\nfrom eddy_paths import *\nfrom copy import deepcopy\n\ng, g_filtered, anticyclonic, cyclonic = identify_eddies(example_file, date)\nax, m = plot_variable(\n    g_filtered, \"adt\", \"Detected Eddies on ADT (m)\", vmin=-0.15, vmax=0.15, cmap=\"Greys\"\n)\nanticyclonic.display(\n    ax, color=\"r\", linewidth=0.75, label=\"Anticyclonic ({nb_obs} eddies)\", ref=-180\n)\ncyclonic.display(\n    ax, color=\"b\", linewidth=0.75, label=\"Cyclonic ({nb_obs} eddies)\", ref=-180\n)\nax.legend()\nupdate_axes(ax)\n\nplt.savefig('/Users/lakshmichetana/ML_eddies_Output/Detected Eddies on ADT (m).png', bbox_inches =\"tight\")\n\n# Plot segmentation mask\nmask = generate_segmentation_mask(\n    g_filtered, anticyclonic, cyclonic, -180, 0, plot=True\n)\nplt.savefig('/Users/lakshmichetana/ML_eddies_Output/Segmentation Mask.png', bbox_inches =\"tight\")",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862075728,
  "history_end_time" : 1675862183946,
  "history_notes" : null,
  "history_process" : "2if9sm",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
},{
  "history_id" : "mvabtluk1fq",
  "history_input" : "#code for plotting segmentation masks, antcyclonic display, cyclonic display and updating the axis\nfrom eddy_plots import *\nfrom eddy_paths import *\nfrom copy import deepcopy\n\n#updated the r4ef details and also the vmin and vmax values\ng, g_filtered, anticyclonic, cyclonic = identify_eddies(example_file, date)\nax, m = plot_variable(\n    g_filtered, \"adt\", \"Detected Eddies on ADT (m)\", vmin=-5, vmax=5, cmap=\"Greys\"\n)\nanticyclonic.display(\n    ax, color=\"r\", linewidth=0.75, label=\"Anticyclonic ({nb_obs} eddies)\", ref=-250\n)\ncyclonic.display(\n    ax, color=\"b\", linewidth=0.75, label=\"Cyclonic ({nb_obs} eddies)\", ref=-250\n)\nax.legend()\nupdate_axes(ax)\n\nplt.savefig('/Users/lakshmichetana/ML_eddies_Output/Detected Eddies on ADT (m)_with_UpdatedVminVmax&RefValues.png', bbox_inches =\"tight\")\n\n# Plot segmentation mask\nmask = generate_segmentation_mask(\n    g_filtered, anticyclonic, cyclonic, -180, 0, plot=True\n)\nplt.savefig(f'{figOutputFolder}/Segmentation Mask_with_UpdatedVminVmax&RefValues.png', bbox_inches =\"tight\")",
  "history_output" : "sh: /usr/lib/python3.6: Is a directory\n",
  "history_begin_time" : 1675862081484,
  "history_end_time" : 1675862183946,
  "history_notes" : null,
  "history_process" : "xm5gfq",
  "host_id" : "z4nr70",
  "indicator" : "Stopped"
}]
